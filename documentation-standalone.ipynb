{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Pre-marshalling Problem Enviroment\n",
    "En el siguiente documento se documentará cómo es que funciona el CPMP Enviroment\n",
    "\n",
    "## Contenidos\n",
    "[Valores Relevantes](#relevant)\n",
    "\n",
    "[Acciones](#actions)\n",
    "\n",
    "[Observaciones](#observation)\n",
    "\n",
    "[Demonstración](#demo)\n",
    "- [Crear el estado y Manipularlo](#createstate)\n",
    "- [Representación del Patio de Containers (yard.py)](#representation)\n",
    "- [Ambiente de CPMP (containeryard.py)](#ambient)\n",
    "- [Pruebas](#test)\n",
    "\n",
    "[Generador de Instancias](#generate)\n",
    "\n",
    "[Ejecutar el Ambiente](#execute)\n",
    "\n",
    "\n",
    "\n",
    "## <a name=\"relevant\"></a> Valores Relevantes\n",
    "\n",
    "state : Es la representación del estado actual, de tipo 'Yard' para permitir realizarle cambios al estado\n",
    "\n",
    "showDebug : Valor booleano que dicta si mostrará información de Debug o no\n",
    "\n",
    "max_step : Pasos máximos, cálculados por una solución greedy. La idea es que la red mejore lo que dicta el greedy en StackedYard.py\n",
    "\n",
    "training :  booleano que dice si está entrenando o no. Más que nada para ver qué conjunto utilizará para trabajar.\n",
    "\n",
    "fileStack :  Los archivos que contienen los estados\n",
    "\n",
    "current_step : Paso actual del estado, la idea es que sea menor a max_step\n",
    "\n",
    "last_reward : la última recompenza obtenida, utilizada para recompenzar la diferencia entre la actual recompenza y la anterior (last_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"actions\"></a>Acciones\n",
    "La acción que entregará como output va a ser un número de 0 a x, dónde x será la anchura del estado\n",
    "\n",
    "Este valor representa el stack en el cuál se debe realizar la acción. En la actualidad, la red identificará un stack y una función de greedy realizará el cambio en dicho stack seleccionado.\n",
    "\n",
    "```py\n",
    "self.action_space = spaces.Discrete(x) # de 0 a x\n",
    "```\n",
    "\n",
    "## <a name=\"observation\"></a>Observaciones\n",
    "La observación será un arreglo que contendrá:\n",
    "\n",
    "1)  El estado actual del patio con los containers, primero los datos vacios y luego los que poseen un valor, de tal manera que los que tienen valor se \"eleven\" y dejen a los ceros arriba. Posterior a esto, se realizaran dos normalizaciones, una que \"Compactará\" los valores, para que siempre esten en rangos iguales, y, una normalización de mayor y menor. Algo así:\n",
    "\n",
    "![Imagen Explicativa de Observacion](https://i.imgur.com/0u1Exr9.png \"Ejemplo Observacion\")\n",
    "\n",
    "Los valores equivalentes al container son numeros enteros pertenecientes a `[0,∞)`. \n",
    "La representación dentro de la observación los normalizara en un rango de `[0,1]`, utilizando una normalización min-max, dónde el minimo y el maximo dependera del  problema que se este resolviendo.\n",
    "\n",
    "2) Además de los valores del estado en sí, se cuenta con un valor booleano (0 ó 1) que representará si una columna se encuentra en orden (1) o no (0).\n",
    "\n",
    "\n",
    "Algo así se vería lo que alimenta a la red.\n",
    "```\n",
    "[0.00833333 0.06666667 0.05       0.01666667 0.025      0.06666667\n",
    " 0.06666667 0.06666667 0.06666667 0.01666667 0.05833333 0.06666667\n",
    " 0.06666667 0.00833333 0.06666667 0.06666667 0.06666667 0.06666667\n",
    " 0.06666667 0.04166667 0.03333333 0.06666667 0.06666667 0.05833333\n",
    " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.05833333\n",
    " 0.05833333 0.06666667 0.06666667 0.         0.06666667 0.06666667\n",
    " 0.06666667 0.06666667 0.06666667 0.03333333 0.03333333 0.06666667\n",
    " 0.06666667 0.05       0.06666667 0.06666667 0.06666667 0.06666667\n",
    " 0.06666667 0.05       0.         1.         1.         0.\n",
    " 1.         1.         1.         1.         1.         0.        ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"demo\"></a>Demonstración\n",
    "A continuación se explicarán funciones del ambiente con ejemplos, de modo que se pueda comprender mejor cómo es que opera.\n",
    "\n",
    "### TensorFlow y Keras\n",
    "Primero que nada debemos tener instalado TensorFlow y Keras. Par que me funcionar la demo tuve que instalar (profe) las siguientes versiones exactas:\n",
    "````\n",
    "pip install tensorflow==1.14.0\n",
    "pip install stable-baselines\n",
    "pip install tensorflow-hub==0.7.0\n",
    "````\n",
    "Suerte!\n",
    "\n",
    "### Carga de librerías básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Métodos para resoción usando greedy y seleccción del stack de destino\n",
    "from containeryard.StackedYard import Layout, greedy_solve, read_file, select_destination_stack\n",
    "from Constants import FILE_PATH\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"representation\"></a>Representación del Patio de Containers (yard.py)\n",
    "Para crear el estado, se utilizará `yard.py`, debido a que `containeryard.py` implementará dicha representación a un formato de gym para facilitar el uso en redes de aprendizaje por reforzamiento.\n",
    "\n",
    "Este archivo será la capa que modificará directamente el estado, el ambiente de gym (`containeryard.py`) la utilizará para no trabajarlo directamente.\n",
    "\n",
    "Los métodos que explicaremos son:\n",
    "\n",
    "`isStackEmpty(i)`: Retornará si el stack i (de 0 a n stacks) se encuentra vacío o no (True/False). Este método será utilizado para saber si es posible sacar un container de dicho stack, ya que si se encuentra vacío no se podrá sacar nada.\n",
    "\n",
    "`isStackFull(i)`: Retornará si el stack i (de 0 a n stacks) se encuentra lleno o no (True/False). Este método será utilizado para saber si es posible poner un container en dicho stack, ya que si se encuentra lleno, no se podrá poner nada sobre este.\n",
    "\n",
    "`moveStack(src, dest)`: Este método utilizará los dos anteriores para ver si es posible realizar una acción. En caso de ser posible, realizará dicha acción y cambiará el estado.\n",
    "\n",
    "`getAsObservation()`: Este método devolverá el estado en formato de observación (sin la normalización), es decir, entregará el estado con los containers NO vacíos primero, y luego pondrá los vacíos, por ejemplo quedaría así: `[4, 3, 6, 7, 4, 4, 0, 0, 0, 0, 0 , ....]`\n",
    "\n",
    "`render(self)`: Este método mostrará el estado de una manera \"humanamente-leíble\"\n",
    "\n",
    "`isSorted(i)`: Verá si un stack i está ordenado o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from containeryard.yard import Yard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generador de instancias aleatorias\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  6  5  2  0]\n",
      " [18  0  0  0  0]\n",
      " [13 13  0  0  0]\n",
      " [13 12 14  2 20]\n",
      " [ 4 18  7  0  0]]\n"
     ]
    }
   ],
   "source": [
    "def random_generator(x=5, y=5, n_containers=15):\n",
    "    max_priority=20\n",
    "\n",
    "    # This generator starts from a solved one and makes random movements.\n",
    "    yard = np.zeros(shape=(x,y), dtype=int)\n",
    "    n_yards = np.zeros(x, dtype=int);\n",
    "\n",
    "    for k in range(n_containers):\n",
    "        i = None\n",
    "        while i==None or n_yards[i] == 5:\n",
    "            i = rand.randint(0,x-1) #se selecciona stack\n",
    "        yard[i][n_yards[i]] = rand.randint(1,max_priority)\n",
    "        n_yards[i] +=1\n",
    "\n",
    "    state = Yard(yard)\n",
    "\n",
    "    layoutState = state.asLayout()\n",
    "    layout = Layout(layoutState, state.y)\n",
    "    max_step = greedy_solve(layout)\n",
    "\n",
    "    return state, layout, max_step \n",
    "\n",
    "arr = random_generator()[0].state\n",
    "print (arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im \n",
    "\n",
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "plt.imshow(arr,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ambient\"></a>Ambiente de CPMP (containeryard.py)\n",
    "ContainerYard se utilizará para que la red pueda tomar la observación y realizar acciones en el estado. Más información sobre gym puede ser leída en su página web: https://gym.openai.com/\n",
    "\n",
    "Los métodos que utilizaremos para explicar serán:\n",
    "`reset()`: Reiniciará el problema actual, cargando otro (En el caso de este ejemplo, siempre cargará uno, pero en la realidad cargará el siguiente!!)\n",
    "\n",
    "`step(action)`: Realizará una acción utilizando _take_action(action), después, calculará la recompensa del estado y entregará la **observación**, **recompensa**, un **boolean** definiendo si el estado es terminal y, opcionalmente, **información de debug**. \n",
    "\n",
    "`_take_action(action)`: Este método realizará la acción en el estado. Primero calculará lo que se demora el greedy en realizar la acción, y lo guardará para calcular la recompensa. Después, utilizará el método de `Yard`, `moveStack` para realizar la acción en el estado.\n",
    "\n",
    "`_next_observation()`: Entregará la observación para la red, descrita anteriormente en el inicio de este archivo\n",
    "\n",
    "`render()`: Este método imprimirá en pantalla el estado actual, de modo que una persona pueda leer en lo que va el estado actualmente. En una versión futura se podrá habilitar para que esta visualización sea gráfica en vez de basada en texto en la terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From containeryard.py\n",
    "class ContainerYard(gym.Env):\n",
    "    metadata = {'render.modes':['human']}\n",
    "\n",
    "    #state : Yard\n",
    "    #showDebug : bool\n",
    "    #max_step : int\n",
    "    #training : bool\n",
    "    #fileStack : list\n",
    "    #current_step : int\n",
    "    #last_reward : int\n",
    "    #x: int\n",
    "    #y: int\n",
    "\n",
    "    episodes=0\n",
    "    \n",
    "    def __init__(self, showDebug = False, x=5, y=5, max_containers=10):\n",
    "        super(ContainerYard, self).__init__()\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.max_containers = max_containers\n",
    "\n",
    "        ### START OF CONFIG ###\n",
    "        self.showDebug = showDebug\n",
    "        self.max_step = 10\n",
    "\n",
    "        #---> Creting the stack for files to use..\n",
    "        self.fileStack = []\n",
    "        \n",
    "        ############################\n",
    "        self.current_step = 0\n",
    "        self.last_reward = 0\n",
    "\n",
    "        # Start The Episode\n",
    "        self.reset()\n",
    "\n",
    "        \"\"\" Action Space: Valores del tamano\n",
    "                vertical del patio.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.state.x)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-1, high=255, shape=(self.state.x*self.state.y + self.state.x ,), dtype=np.float_)\n",
    "\n",
    "\n",
    "    def _loadStack(self, path):\n",
    "        for subdir, _, files in os.walk(FILE_PATH + path):\n",
    "            for file in files:\n",
    "                if \"optimo\" not in file:\n",
    "                    full = subdir + os.sep + file\n",
    "                    if os.path.isfile(full):\n",
    "                        self.fileStack.append(full)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \n",
    "        #Normalization and Generating the Yard Observation\n",
    "        obs = self.state.getAsObservation()\n",
    "                \n",
    "        #Misc Observation\n",
    "        for i in range(self.state.x):\n",
    "            obs = np.insert(obs, obs.size, 1 if self.state.isSorted(i) is True else 0)\n",
    "        \n",
    "        #self.state.render()\n",
    "        return obs\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "\n",
    "        layoutState = self.state.asLayout()\n",
    "        dest = select_destination_stack(\n",
    "            Layout(layoutState, self.state.y), \n",
    "            action\n",
    "        )\n",
    "        if dest==None: return None\n",
    "        return self.state.moveStack(action, dest)\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #Taking Action!\n",
    "        ret = self._take_action(action)\n",
    "\n",
    "        #New Greedy Value.\n",
    "        self.greedy_steps = greedy_solve(\n",
    "            Layout(self.state.asLayout(), self.state.y)\n",
    "        )\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        formula_reward = np.power(0.95, self.current_step + self.greedy_steps)\n",
    "        reward = 100*(formula_reward - self.last_reward)\n",
    "        self.last_reward = formula_reward\n",
    "        \n",
    "        \n",
    "        #if self.state.isDone(): reward=1\n",
    "\n",
    "        done = (self.state.isDone() or self.current_step >= self.max_step)\n",
    "\n",
    "        if ret == None:\n",
    "            #Could not make action, so we punish it.\n",
    "            reward = -10\n",
    "            done = True\n",
    "    \n",
    "\n",
    "        ##################\n",
    "        ##  DEBUG INFO  ##  \n",
    "        ##################\n",
    "        if self.showDebug is True:\n",
    "            info ={\n",
    "                \"current_step\" : self.current_step,\n",
    "                \"greedy_steps\" : self.greedy_steps,\n",
    "                \"max_step\" : self.max_step,\n",
    "                \"reward\" : reward,\n",
    "                \"current_action\" : action,\n",
    "                \"action_status\" : str(ret),\n",
    "                \"stackSize\" : len(self.fileStack),\n",
    "                \"sorted\" : str(self.state.isDone()),\n",
    "            }\n",
    "        else:\n",
    "            info = None\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, info\n",
    "            \n",
    "\n",
    "    def reset(self):\n",
    "        #Resetting\n",
    "        ContainerYard.episodes += 1\n",
    "        self.current_step = 0\n",
    "        self.greedy_steps  = 0\n",
    "        while self.greedy_steps  == 0  :\n",
    "            self.state, self.layout, self.max_step = \\\n",
    "                random_generator(x=self.x, y=self.y, n_containers=self.max_containers)\n",
    "            self.greedy_steps  = greedy_solve( Layout(self.state.asLayout(), self.state.y) )\n",
    "        \n",
    "        self.last_reward = np.power(0.95, self.greedy_steps)\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode=None, test=False):\n",
    "        self.state.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"test\"></a>Pruebas\n",
    "Aquí realizaremos pruebas para que se comprenda el funcionamiento.\n",
    "\n",
    "Iniciaremos el estado y probaremos sus funcionamientos básicos.\n",
    "\n",
    "`gym_representation` será nuestro ambiente de gym, primero mostraremos con `.state.render()` cómo es que verá la persona el ambiente y con `._next_observation()` lo que ve la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARA HUMANOS\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [20  0  8  0  0]\n",
      " [ 9  8 14  0  0]\n",
      " [20  1 17 10 10]]\n",
      "Estado transformado: (Como arreglo)\n",
      "[1.         0.28571429 0.28571429 0.57142857 0.57142857 0.42857143\n",
      " 0.14285714 0.71428571 1.         1.         1.         1.\n",
      " 0.85714286 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observacion (anterior + 0/1 si stack esta ordenado o no)\n",
      "[1.         0.28571429 0.28571429 0.57142857 0.57142857 0.42857143\n",
      " 0.14285714 0.71428571 1.         1.         1.         1.\n",
      " 0.85714286 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.         0.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im \n",
    "\n",
    "gym_representation = ContainerYard()\n",
    "\n",
    "print('PARA HUMANOS')\n",
    "gym_representation.state.render()\n",
    "\n",
    "print('Estado transformado: (Como arreglo)')\n",
    "print(gym_representation.state.getAsObservation())\n",
    "\n",
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "\n",
    "arr=gym_representation.state.getAsObservation()\n",
    "arr.shape=(5,5)\n",
    "\n",
    "plt.imshow(arr,cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('\\nObservacion (anterior + 0/1 si stack esta ordenado o no)')\n",
    "print(gym_representation._next_observation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, probaremos un par de funciones.\n",
    "Comenzaremos probando realizar un movimiento sobre la columna numero 3 (Si contamos desde 0, sería la número 2.)\n",
    "\n",
    "El método retornara cuatro valores:\n",
    "La nueva observación, la recompensa, si el estado es terminal y, por último, información adicional opcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información de Retorno: (Observacion, recompensa, es estado terminal?, información debug [OPCIONAL])\n",
      "(array([1.        , 0.14285714, 0.28571429, 0.57142857, 0.57142857,\n",
      "       0.42857143, 1.        , 0.28571429, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.71428571, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.85714286, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.        , 1.        , 1.        , 1.        , 1.        ]), 0.0, False, None)\n",
      "\n",
      "Render:\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  8  0  0]\n",
      " [20  0  8  0  0]\n",
      " [ 9  0 14  0  0]\n",
      " [20  1 17 10 10]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Información de Retorno: (Observacion, recompensa, es estado terminal?, información debug [OPCIONAL])\")\n",
    "print(gym_representation.step(1))\n",
    "\n",
    "print(\"\\nRender:\")\n",
    "gym_representation.state.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizaremos los métodos que se utilizan dentro de todas las funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacks Ordenados: \n",
      "[False  True  True  True  True]\n",
      "\n",
      "Es Estado Terminal: \n",
      "False\n",
      "\n",
      "Valor Greedy Estado Actual - Pasos Tomados: \n",
      "2 1\n",
      "\n",
      "Estado Actual:\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  8  0  0]\n",
      " [20  0  8  0  0]\n",
      " [ 9  0 14  0  0]\n",
      " [20  1 17 10 10]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Stacks Ordenados: \")\n",
    "print(gym_representation.state.getAllSorts())\n",
    "\n",
    "\n",
    "print(\"\\nEs Estado Terminal: \")\n",
    "print(gym_representation.state.isDone())\n",
    "\n",
    "print(\"\\nValor Greedy Estado Actual - Pasos Tomados: \")\n",
    "print(greedy_solve(Layout(gym_representation.state.asLayout(), gym_representation.state.y)), gym_representation.current_step)\n",
    "\n",
    "print('\\nEstado Actual:')\n",
    "gym_representation.state.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"execute\"></a>Ejecutar el Ambiente\n",
    "Un ejemplo de la ejecucion del ambiente puede ser vista en el archivo [DQN_Optimizer.py](https://github.com/Nyuku/CPMP-AI/tree/master/DQN_Optimizer.py). En esta demonstracion se utilizara la libreria `stable_baselines`, de modo que solo nos fijemos en el ambiente y su utilizacion.\n",
    "\n",
    "### Configuracion\n",
    "Primero, tenemos los `timesteps`. Esta variable definirá la cantidad de pasos que realizará la red para aprender, mientras más mejor, claro.\n",
    "Segundo, tenemos la configuración del ambiente. El Objeto de ambiente se instanciará con los siguientes aributos:\n",
    "\n",
    "- `showDebug` : Entregará información adicional de Debug, como la recompensa en el estado actual, la cantidad de pasos que ha tomado y los máximos, etc.\n",
    "- `x` : Ancho del problema\n",
    "- `y` : Alto del Problema\n",
    "\n",
    "Un ejemplo de instanciación del problema sería: `ContainerYard(showDebug = True)`, en este ejemplo, se mostrará la información de debug y se utilizarán las dimensiones por defecto del problema: 20x5.\n",
    "\n",
    "\n",
    "### Utilizar el Ambiente\n",
    "Para utilizar el ambiente se tendra que tener en cuenta las funciones importantes del ambiente. En estas tenemos:\n",
    "\n",
    "`reset`: este metodo reinicia el problema actual para dar paso al siguiente, retorna una observacion en formato red para alimentar a esta.\n",
    "\n",
    "`step`: este metodo recibe una accion en formato entero, de 1 a x, donde x sera el ancho del problema. Step deolvera, al igual que reset, una observacion del ambiente. Ademas, devolvera la recompensa de la accion, un boolean que dictamina si es un paso final o no y, opcionalmente, una informacion de debug.\n",
    "\n",
    "`render`: este metodo mostrara por pantalla el estado actual en la terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#   Se cargan las librerias que se utilizaran para probar el ambiente.  \n",
    "#   Se utilizara DQN para probar.\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "#   Cargamos el Script que contiene el ambiente\n",
    "#import containeryard.containeryard\n",
    "#from containeryard.containeryard import ContainerYard\n",
    "#import importlib\n",
    "#importlib.reload(containeryard.containeryard)\n",
    "\n",
    "\n",
    "#Para evitar que sea mucho tiempo, ejecutaremos 1000 veces, solo para que se vea como funciona.\n",
    "timesteps = 40000\n",
    "\n",
    "# Se crea el ambiente:\n",
    "# showDebug -> Permite ver la informacion de debug que se guardara en la variable info abajo.\n",
    "# training -> Los generadores crean conjuntos d prueba y entrenamiento, para evitar que este utilice los mismos\n",
    "#             ejemplos en cada momento. Aqui define que utilizara solo el conjunto de entrenamiento\n",
    "env = DummyVecEnv([lambda: ContainerYard(showDebug = True, x=5, y=5, max_containers=10)])\n",
    "\n",
    "#Se crea el modelo, utilizando stable_baselines.\n",
    "model = DQN(MlpPolicy, env, verbose=0)\n",
    "\n",
    "#Aqui se entrena!\n",
    "model.learn(total_timesteps=timesteps)\n",
    "\n",
    "\n",
    "for i_episode in range(10): # Se probaran 5 problemas diferentes.\n",
    "    cur_step = 0\n",
    "\n",
    "    #Comenzamos con un reset, para tomar el siguiente problema.\n",
    "    print(\"========= Reset =========\")\n",
    "    print(\"Problema Numero \",str(i_episode+1))\n",
    "    print(\"=========================\")\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    env.render()\n",
    "    print(\"=====COMENZANDO A RESOLVER=====\")\n",
    "\n",
    "    while True:\n",
    "        # El modelo usara la funcion para predecir cual es la mejor\n",
    "        # accion para la observacion \"obs\"\n",
    "        act, _states = model.predict(obs)\n",
    "\n",
    "        #Se utilizara la accion \"act\" que entrego la red para ser utilizada en la\n",
    "        #funcion \"step\". Esta devolvera una observacion, una recompensa, un boolean que dira\n",
    "        #si finalizo el episodio y, finalmente informacion de debug en \"info\".\n",
    "        obs, reward, done, info = env.step(act)\n",
    "\n",
    "        #mostrara en la terminal el estado actual.\n",
    "        env.render()\n",
    "        cur_step += 1\n",
    "        print(\"PASO NUMERO {0}\".format(cur_step))\n",
    "        print(\"SE ELIGIO EL STACK {0}\\n\".format(act))\n",
    "        print(\"LA RECOMPENSA ES DE {0}\\n\".format(reward))\n",
    "        print(\"STEPS {0}\\n\".format(info))\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            # Si es terminal, entonces se termina y se comienza con el siguiente problema.\n",
    "            break\n",
    "\n",
    "print(\"Se completaron los problemas!\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
